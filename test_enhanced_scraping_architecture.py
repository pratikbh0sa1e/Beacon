"""
Test script for the 4 enhanced scraping architectural improvements:
1. Site-specific scrapers
2. Sliding window re-scanning
3. Page content hashing
4. Enhanced document identity
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from Agent.web_scraping.site_scrapers import get_scraper_for_site, get_available_scrapers
from Agent.web_scraping.sliding_window_manager import SlidingWindowManager
from Agent.web_scraping.page_hash_tracker import PageHashTracker
from Agent.web_scraping.document_identity_manager import DocumentIdentityManager
from Agent.web_scraping.enhanced_scraping_orchestrator import EnhancedScrapingOrchestrator
from bs4 import BeautifulSoup
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_site_specific_scrapers():
    """Test site-specific scrapers"""
    print("\n" + "="*60)
    print("TESTING SITE-SPECIFIC SCRAPERS")
    print("="*60)
    
    # Test scraper registry
    available_scrapers = get_available_scrapers()
    print(f"Available scrapers: {available_scrapers}")
    
    # Test getting scrapers for different sites
    test_sites = ['moe', 'ugc', 'aicte', 'generic']
    
    for site_type in test_sites:
        scraper = get_scraper_for_site(site_type)
        site_info = scraper.get_site_info()
        
        print(f"\n{site_type.upper()} Scraper:")
        print(f"  Class: {site_info['scraper_class']}")
        print(f"  Site Name: {site_info['site_name']}")
        print(f"  Extensions: {site_info['supported_extensions']}")
        print(f"  Rate Limit: {site_info['rate_limit_delay']}s")
        
        if 'specialization' in site_info:
            print(f"  Specialization: {site_info['specialization']}")
    
    print("\n‚úÖ Site-specific scrapers test completed")


def test_page_content_hashing():
    """Test page content hashing system"""
    print("\n" + "="*60)
    print("TESTING PAGE CONTENT HASHING")
    print("="*60)
    
    hash_tracker = PageHashTracker()
    
    # Create test HTML content
    test_html1 = """
    <html>
        <head><title>Test Page</title></head>
        <body>
            <nav>Navigation menu</nav>
            <main>
                <h1>Important Document</h1>
                <p>This is the main content that matters.</p>
                <div class="timestamp">Last updated: 2024-01-15 10:30:00</div>
            </main>
            <footer>Footer content</footer>
            <script>console.log('dynamic script');</script>
        </body>
    </html>
    """
    
    test_html2 = """
    <html>
        <head><title>Test Page</title></head>
        <body>
            <nav>Different navigation</nav>
            <main>
                <h1>Important Document</h1>
                <p>This is the main content that matters.</p>
                <div class="timestamp">Last updated: 2024-01-15 11:45:00</div>
            </main>
            <footer>Different footer</footer>
            <script>console.log('different script');</script>
        </body>
    </html>
    """
    
    test_html3 = """
    <html>
        <head><title>Test Page</title></head>
        <body>
            <nav>Navigation menu</nav>
            <main>
                <h1>Important Document</h1>
                <p>This is DIFFERENT main content that matters.</p>
                <div class="timestamp">Last updated: 2024-01-15 10:30:00</div>
            </main>
            <footer>Footer content</footer>
        </body>
    </html>
    """
    
    # Test hash calculation
    soup1 = BeautifulSoup(test_html1, 'html.parser')
    soup2 = BeautifulSoup(test_html2, 'html.parser')
    soup3 = BeautifulSoup(test_html3, 'html.parser')
    
    hash1 = hash_tracker.calculate_page_hash(soup1, "test1.html")
    hash2 = hash_tracker.calculate_page_hash(soup2, "test2.html")
    hash3 = hash_tracker.calculate_page_hash(soup3, "test3.html")
    
    print(f"Hash 1 (original): {hash1[:16]}...")
    print(f"Hash 2 (same content, different dynamic elements): {hash2[:16]}...")
    print(f"Hash 3 (different content): {hash3[:16]}...")
    
    # Test hash comparison
    if hash1 == hash2:
        print("‚úÖ Hashes match for same content (dynamic elements ignored)")
    else:
        print("‚ùå Hashes don't match for same content")
    
    if hash1 != hash3:
        print("‚úÖ Hashes differ for different content")
    else:
        print("‚ùå Hashes are same for different content")
    
    print("\n‚úÖ Page content hashing test completed")


def test_document_identity_manager():
    """Test enhanced document identity checking"""
    print("\n" + "="*60)
    print("TESTING ENHANCED DOCUMENT IDENTITY")
    print("="*60)
    
    identity_manager = DocumentIdentityManager()
    
    # Test URL normalization
    test_urls = [
        "https://example.com/doc.pdf",
        "https://example.com/doc.pdf?utm_source=google",
        "https://example.com/doc.pdf?ref=homepage",
        "https://example.com/doc.pdf#section1"
    ]
    
    print("URL Normalization Test:")
    for url in test_urls:
        normalized = identity_manager._normalize_url(url)
        print(f"  {url} -> {normalized}")
    
    # Test URL equivalence
    url1 = "https://example.com/doc.pdf?page=1&sort=date"
    url2 = "https://example.com/doc.pdf?sort=date&page=1"
    url3 = "https://example.com/doc.pdf?page=1&sort=date&utm_source=google"
    
    print(f"\nURL Equivalence Test:")
    print(f"  URL1: {url1}")
    print(f"  URL2: {url2}")
    print(f"  URL3: {url3}")
    print(f"  URL1 == URL2: {identity_manager._urls_are_equivalent(url1, url2)}")
    print(f"  URL1 == URL3: {identity_manager._urls_are_equivalent(url1, url3)}")
    
    # Test content hashing
    content1 = "This is a test document with some content."
    content2 = "This   is  a   test document    with some content."  # Different whitespace
    content3 = "This is a different document with other content."
    
    hash1 = identity_manager._calculate_content_hash(content1)
    hash2 = identity_manager._calculate_content_hash(content2)
    hash3 = identity_manager._calculate_content_hash(content3)
    
    print(f"\nContent Hashing Test:")
    print(f"  Content 1 hash: {hash1[:16]}...")
    print(f"  Content 2 hash (normalized): {hash2[:16]}...")
    print(f"  Content 3 hash (different): {hash3[:16]}...")
    print(f"  Hash1 == Hash2 (normalized): {hash1 == hash2}")
    print(f"  Hash1 != Hash3 (different): {hash1 != hash3}")
    
    print("\n‚úÖ Enhanced document identity test completed")


def test_sliding_window_manager():
    """Test sliding window re-scanning logic"""
    print("\n" + "="*60)
    print("TESTING SLIDING WINDOW RE-SCANNING")
    print("="*60)
    
    window_manager = SlidingWindowManager(window_size=3)
    
    print(f"Sliding window size: {window_manager.window_size}")
    
    # Test page URL construction
    base_url = "https://example.com/documents"
    
    print(f"\nPage URL Construction Test:")
    for i in range(1, 6):
        page_url = window_manager._get_page_url(base_url, i)
        print(f"  Page {i}: {page_url}")
    
    # Test with query parameters
    base_url_with_params = "https://example.com/documents?category=policy"
    
    print(f"\nPage URL with existing params:")
    for i in range(1, 4):
        page_url = window_manager._get_page_url(base_url_with_params, i)
        print(f"  Page {i}: {page_url}")
    
    print("\n‚úÖ Sliding window manager test completed")


def test_enhanced_orchestrator():
    """Test the complete enhanced scraping orchestrator"""
    print("\n" + "="*60)
    print("TESTING ENHANCED SCRAPING ORCHESTRATOR")
    print("="*60)
    
    orchestrator = EnhancedScrapingOrchestrator(window_size=3)
    
    print("Enhanced Scraping Orchestrator initialized with:")
    print(f"  Sliding Window Manager: ‚úÖ")
    print(f"  Page Hash Tracker: ‚úÖ")
    print(f"  Document Identity Manager: ‚úÖ")
    
    # Test statistics (without database)
    try:
        stats = orchestrator.get_orchestrator_statistics()
        print(f"\nOrchestrator Statistics:")
        print(f"  Timestamp: {stats.get('timestamp', 'N/A')}")
        print(f"  Components initialized: ‚úÖ")
    except Exception as e:
        print(f"  Statistics test skipped (no database): {str(e)}")
    
    print("\n‚úÖ Enhanced orchestrator test completed")


def test_integration():
    """Test integration between all components"""
    print("\n" + "="*60)
    print("TESTING COMPONENT INTEGRATION")
    print("="*60)
    
    # Test that all components can be imported and initialized together
    try:
        # Initialize all components
        scraper = get_scraper_for_site('moe')
        window_manager = SlidingWindowManager(window_size=3)
        hash_tracker = PageHashTracker()
        identity_manager = DocumentIdentityManager()
        orchestrator = EnhancedScrapingOrchestrator(window_size=3)
        
        print("‚úÖ All components initialized successfully")
        
        # Test that they can work together
        test_html = "<html><body><h1>Test</h1><a href='doc.pdf'>Document</a></body></html>"
        soup = BeautifulSoup(test_html, 'html.parser')
        
        # Test scraper
        documents = scraper.get_document_links(soup, "https://example.com")
        print(f"‚úÖ Scraper found {len(documents)} documents")
        
        # Test hash tracker
        page_hash = hash_tracker.calculate_page_hash(soup, "https://example.com")
        print(f"‚úÖ Hash tracker calculated hash: {page_hash[:16]}...")
        
        # Test identity manager
        content_hash = identity_manager._calculate_content_hash("test content")
        print(f"‚úÖ Identity manager calculated content hash: {content_hash[:16]}...")
        
        print("\n‚úÖ Component integration test completed")
        
    except Exception as e:
        print(f"‚ùå Integration test failed: {str(e)}")


def main():
    """Run all tests"""
    print("ENHANCED SCRAPING ARCHITECTURE TESTS")
    print("="*60)
    print("Testing the 4 architectural improvements:")
    print("1. Site-specific scrapers")
    print("2. Sliding window re-scanning")
    print("3. Page content hashing")
    print("4. Enhanced document identity")
    
    try:
        test_site_specific_scrapers()
        test_page_content_hashing()
        test_document_identity_manager()
        test_sliding_window_manager()
        test_enhanced_orchestrator()
        test_integration()
        
        print("\n" + "="*60)
        print("üéâ ALL TESTS COMPLETED SUCCESSFULLY!")
        print("="*60)
        print("\nThe enhanced scraping architecture is ready with:")
        print("‚úÖ Site-specific scrapers (MoE, UGC, AICTE)")
        print("‚úÖ Sliding window re-scanning (configurable window size)")
        print("‚úÖ Page content hashing (ignores dynamic elements)")
        print("‚úÖ Enhanced document identity (URL-first approach)")
        print("‚úÖ Integrated orchestrator (combines all improvements)")
        
        print("\nNext steps:")
        print("1. Test with real government websites")
        print("2. Configure database connections")
        print("3. Integrate with existing web scraping endpoints")
        print("4. Monitor performance and adjust parameters")
        
    except Exception as e:
        print(f"\n‚ùå Test failed: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()