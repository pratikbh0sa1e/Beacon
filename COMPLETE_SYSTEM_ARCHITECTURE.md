# ğŸ—ï¸ BEACON Platform - Complete System Architecture

## Table of Contents

1. [System Overview](#system-overview)
2. [Core Components](#core-components)
3. [Feature Breakdown](#feature-breakdown)
4. [File Structure](#file-structure)
5. [Detailed Workflows](#detailed-workflows)

---

## System Overview

**BEACON** is an AI-powered document management and RAG (Retrieval-Augmented Generation) system for government education policies.

### Key Features:

- ğŸŒ **Web Scraping**: Automated document collection from government websites
- ğŸ“„ **Document Management**: Upload, process, and organize documents
- ğŸ¤– **AI Chat**: Ask questions and get answers from documents
- ğŸ‘¥ **Role-Based Access**: Different permissions for different user types
- ğŸ” **Smart Search**: Vector + keyword hybrid search
- ğŸ“Š **Document Families**: Version tracking and deduplication

### Tech Stack:

- **Backend**: FastAPI (Python)
- **Frontend**: React + Vite
- **Database**: PostgreSQL + pgvector
- **AI Models**: Google Gemini, OpenRouter (Llama 3.3)
- **Storage**: Supabase S3
- **Embeddings**: BGE-M3 (multilingual)

---

## Core Components

### 1. Backend API (`backend/`)

**Purpose**: REST API for all operations

**Key Files**:

- `backend/main.py` - FastAPI app initialization, CORS, routes
- `backend/database.py` - SQLAlchemy models (User, Document, Institution, etc.)
- `backend/routers/auth_router.py` - Login, signup, email verification
- `backend/routers/document_router.py` - Document upload, download, approval
- `backend/routers/enhanced_web_scraping_router.py` - Web scraping endpoints
- `backend/routers/chat_router.py` - AI chat with documents

**What it does**:

- Handles HTTP requests from frontend
- Manages database operations
- Enforces role-based access control
- Coordinates AI agents and tools

---

### 2. Web Scraping System (`Agent/web_scraping/`)

**Purpose**: Automatically scrape documents from government websites

**Key Files**:

- `Agent/web_scraping/enhanced_processor.py` - Main scraping orchestrator
- `Agent/web_scraping/site_scrapers/moe_scraper.py` - Ministry of Education scraper
- `Agent/web_scraping/site_scrapers/ugc_scraper.py` - UGC scraper
- `Agent/web_scraping/site_scrapers/aicte_scraper.py` - AICTE scraper
- `Agent/web_scraping/document_identity_manager.py` - Deduplication logic
- `Agent/web_scraping/sliding_window_manager.py` - Pagination handling
- `Agent/web_scraping/session_storage.py` - Persist scraping sessions

**What it does**:

1. Visits government websites
2. Finds document links (PDFs, DOCs)
3. Downloads documents
4. Uploads to Supabase S3
5. Extracts metadata using AI
6. Saves to database
7. Detects duplicates and updates

**Deduplication Strategy** (3 levels):

- URL-based: Same URL = duplicate
- Content hash: Same SHA256 = duplicate
- Normalized URL: Similar URLs = duplicate

---

### 3. Metadata Extraction (`Agent/metadata/`)

**Purpose**: Extract structured information from documents using AI

**Key Files**:

- `Agent/metadata/extractor.py` - AI-powered metadata extraction
- `Agent/metadata/reranker.py` - Rerank search results by relevance

**What it does**:

- Extracts: title, department, document type, date, summary, keywords
- Uses: Google Gemini (gemma-3-12b) - 14,400 requests/day
- Fallback: OpenRouter if Gemini fails
- Quality validation: Ensures metadata meets standards

**Metadata Fields Extracted**:

```json
{
  "title": "National Education Policy 2020",
  "department": "Ministry of Education",
  "document_type": "policy",
  "date_published": "2020-07-29",
  "keywords": ["education", "policy", "reform"],
  "summary": "Comprehensive education reform...",
  "key_topics": ["curriculum", "teacher training"],
  "entities": {
    "departments": ["MoE", "UGC"],
    "locations": ["India"],
    "people": ["Minister"]
  }
}
```

---

### 4. Embeddings & Vector Search (`Agent/embeddings/`, `Agent/vector_store/`)

**Purpose**: Convert text to vectors for semantic search

**Key Files**:

- `Agent/embeddings/bge_embedder.py` - BGE-M3 multilingual embeddings
- `Agent/vector_store/pgvector_store.py` - PostgreSQL vector storage
- `Agent/chunking/adaptive_chunker.py` - Smart text chunking

**What it does**:

1. Splits documents into chunks (adaptive size)
2. Generates 1024-dim embeddings (BGE-M3)
3. Stores in PostgreSQL with pgvector
4. Enables semantic search (cosine similarity)

**Embedding Model**: BAAI/bge-m3

- Dimension: 1024
- Languages: 100+ (Hindi, Tamil, Telugu, Bengali, etc.)
- Use case: Multilingual document search

---

### 5. Lazy RAG System (`Agent/lazy_rag/`)

**Purpose**: Embed documents on-demand to save compute

**Key Files**:

- `Agent/lazy_rag/lazy_embedder.py` - On-demand embedding
- `Agent/tools/lazy_search_tools.py` - Search with lazy embedding

**How it works**:

1. User searches for "Indo-Norwegian program"
2. System checks: Are relevant docs embedded?
3. If NO: Rank unembed docs by metadata (BM25)
4. Embed top 3 most relevant docs
5. Search pgvector for matches
6. Return results

**Benefits**:

- First search: Slower (needs embedding)
- Subsequent searches: Fast (uses cached embeddings)
- Saves compute: Only embed what's needed

---

### 6. RAG Agent (`Agent/rag_agent/`)

**Purpose**: Answer questions using documents

**Key Files**:

- `Agent/rag_agent/react_agent.py` - ReAct agent with tools
- `Agent/rag_enhanced/family_aware_retriever.py` - Document family search

**What it does**:

1. User asks: "What is the Indo-Norwegian program?"
2. Agent searches documents
3. Finds relevant chunks
4. Generates answer using LLM
5. Cites sources with document IDs

**LLM Used**: OpenRouter (Llama 3.3 70B)

- 200 requests/day (FREE)
- Excellent for Q&A
- No API version issues

**Tools Available**:

- `search_documents_lazy()` - Search all documents
- `search_specific_document_lazy()` - Search one document
- `enhanced_search_documents()` - Family-aware search

---

### 7. Document Families (`Agent/document_families/`)

**Purpose**: Group related documents and track versions

**Key Files**:

- `Agent/document_families/family_manager.py` - Family management

**What it does**:

- Groups: "NEP 2020 v1", "NEP 2020 v2" â†’ Same family
- Tracks: Version numbers, superseded documents
- Detects: Updates to existing documents
- Improves RAG: Retrieves latest version

**Example Family**:

```
Family: "National Education Policy"
â”œâ”€â”€ NEP 2020 Draft (v1.0) - Superseded
â”œâ”€â”€ NEP 2020 Final (v2.0) - Latest âœ…
â””â”€â”€ NEP 2020 Amendment (v2.1) - Latest âœ…
```

---

### 8. Frontend (`frontend/src/`)

**Purpose**: User interface for all features

**Key Files**:

- `frontend/src/App.jsx` - Main app, routing
- `frontend/src/pages/admin/EnhancedWebScrapingPage.jsx` - Scraping UI
- `frontend/src/pages/admin/DocumentManagementPage.jsx` - Document management
- `frontend/src/pages/ChatPage.jsx` - AI chat interface
- `frontend/src/pages/auth/LoginPage.jsx` - Login/signup
- `frontend/src/components/layout/Sidebar.jsx` - Navigation

**What it does**:

- Provides UI for all features
- Handles authentication
- Displays documents, stats, logs
- Real-time scraping progress
- Chat interface with citations

---

## Feature Breakdown

### Feature 1: Web Scraping

**Files Involved**:

1. `frontend/src/pages/admin/EnhancedWebScrapingPage.jsx` - UI
2. `backend/routers/enhanced_web_scraping_router.py` - API
3. `Agent/web_scraping/enhanced_processor.py` - Scraping logic
4. `Agent/web_scraping/site_scrapers/moe_scraper.py` - Site-specific scraper
5. `Agent/metadata/extractor.py` - Metadata extraction
6. `backend/utils/supabase_storage.py` - S3 upload

**Flow**:

```
User clicks "Scrape" â†’ API call â†’ enhanced_processor.py
â†’ Site scraper finds links â†’ Downloads PDFs
â†’ Uploads to S3 â†’ Extracts metadata (AI)
â†’ Saves to database â†’ Returns stats
```

**Key Features**:

- Multi-page pagination (up to 100 pages)
- Deduplication (3 levels)
- Stop button (graceful shutdown)
- Progress logging
- Family detection

---

### Feature 2: Document Upload

**Files Involved**:

1. `frontend/src/pages/admin/DocumentManagementPage.jsx` - UI
2. `backend/routers/document_router.py` - API
3. `backend/utils/text_extractor.py` - Text extraction
4. `backend/utils/supabase_storage.py` - S3 upload
5. `Agent/metadata/extractor.py` - Metadata extraction

**Flow**:

```
User uploads PDF â†’ API receives file
â†’ Extracts text (PyPDF2/Tesseract OCR)
â†’ Uploads to S3 â†’ Extracts metadata
â†’ Saves to database â†’ Returns document ID
```

**Supported Formats**:

- PDF (with OCR fallback)
- DOCX, DOC
- TXT
- PPTX

---

### Feature 3: AI Chat (RAG)

**Files Involved**:

1. `frontend/src/pages/ChatPage.jsx` - Chat UI
2. `backend/routers/chat_router.py` - Chat API
3. `Agent/rag_agent/react_agent.py` - RAG agent
4. `Agent/tools/lazy_search_tools.py` - Search tools
5. `Agent/lazy_rag/lazy_embedder.py` - Lazy embedding
6. `Agent/vector_store/pgvector_store.py` - Vector search
7. `Agent/metadata/reranker.py` - Result reranking

**Flow**:

```
User asks question â†’ Chat API
â†’ RAG agent processes query
â†’ Searches documents (lazy embedding if needed)
â†’ Retrieves relevant chunks
â†’ LLM generates answer with citations
â†’ Returns formatted response
```

**Search Strategy**:

1. Check if docs are embedded
2. If not: Rank by metadata, embed top 3
3. Vector search (70%) + BM25 (30%)
4. Rerank results by relevance
5. Return top 5 chunks

---

### Feature 4: Role-Based Access Control

**Files Involved**:

1. `backend/routers/auth_router.py` - Authentication
2. `backend/database.py` - User model
3. `backend/constants/roles.py` - Role definitions
4. All routers - Access checks

**Roles**:

- **Developer**: Full access to everything
- **Ministry Admin**: Access to all public + ministry docs
- **University Admin**: Access to public + own institution docs
- **Document Officer**: Upload and manage docs
- **Student**: Read-only access to approved docs
- **Public Viewer**: Public documents only

**How it works**:

```python
# In every API endpoint
current_user = get_current_user(token)
if current_user.role != "developer":
    # Filter documents by role
    query = query.filter(visibility_level="public")
```

---

### Feature 5: Document Families & Versioning

**Files Involved**:

1. `Agent/document_families/family_manager.py` - Family logic
2. `backend/database.py` - DocumentFamily model
3. `Agent/web_scraping/enhanced_processor.py` - Family detection
4. `Agent/rag_enhanced/family_aware_retriever.py` - Family search

**How it works**:

1. Document scraped: "NEP 2020 Final.pdf"
2. Check: Does family exist for "NEP 2020"?
3. If YES: Add to family, increment version
4. If NO: Create new family
5. Mark previous version as superseded
6. Update family centroid embedding

**Benefits**:

- Avoid duplicate results in search
- Always retrieve latest version
- Track document evolution
- Better RAG accuracy

---

## File Structure

```
BEACON/
â”œâ”€â”€ backend/                          # Backend API
â”‚   â”œâ”€â”€ main.py                       # FastAPI app
â”‚   â”œâ”€â”€ database.py                   # Database models
â”‚   â”œâ”€â”€ routers/                      # API endpoints
â”‚   â”‚   â”œâ”€â”€ auth_router.py           # Authentication
â”‚   â”‚   â”œâ”€â”€ document_router.py       # Document management
â”‚   â”‚   â”œâ”€â”€ chat_router.py           # AI chat
â”‚   â”‚   â””â”€â”€ enhanced_web_scraping_router.py  # Web scraping
â”‚   â”œâ”€â”€ utils/                        # Utilities
â”‚   â”‚   â”œâ”€â”€ text_extractor.py        # Text extraction
â”‚   â”‚   â””â”€â”€ supabase_storage.py      # S3 upload
â”‚   â””â”€â”€ constants/                    # Constants
â”‚       â””â”€â”€ roles.py                  # Role definitions
â”‚
â”œâ”€â”€ Agent/                            # AI Agent System
â”‚   â”œâ”€â”€ web_scraping/                # Web scraping
â”‚   â”‚   â”œâ”€â”€ enhanced_processor.py    # Main scraper
â”‚   â”‚   â”œâ”€â”€ site_scrapers/           # Site-specific scrapers
â”‚   â”‚   â”‚   â”œâ”€â”€ moe_scraper.py      # Ministry of Education
â”‚   â”‚   â”‚   â”œâ”€â”€ ugc_scraper.py      # UGC
â”‚   â”‚   â”‚   â””â”€â”€ aicte_scraper.py    # AICTE
â”‚   â”‚   â”œâ”€â”€ document_identity_manager.py  # Deduplication
â”‚   â”‚   â””â”€â”€ sliding_window_manager.py     # Pagination
â”‚   â”‚
â”‚   â”œâ”€â”€ metadata/                    # Metadata extraction
â”‚   â”‚   â”œâ”€â”€ extractor.py            # AI metadata extraction
â”‚   â”‚   â””â”€â”€ reranker.py             # Result reranking
â”‚   â”‚
â”‚   â”œâ”€â”€ embeddings/                  # Embeddings
â”‚   â”‚   â”œâ”€â”€ bge_embedder.py         # BGE-M3 embeddings
â”‚   â”‚   â””â”€â”€ embedding_config.py     # Config
â”‚   â”‚
â”‚   â”œâ”€â”€ vector_store/                # Vector storage
â”‚   â”‚   â””â”€â”€ pgvector_store.py       # PostgreSQL + pgvector
â”‚   â”‚
â”‚   â”œâ”€â”€ lazy_rag/                    # Lazy RAG
â”‚   â”‚   â””â”€â”€ lazy_embedder.py        # On-demand embedding
â”‚   â”‚
â”‚   â”œâ”€â”€ rag_agent/                   # RAG agent
â”‚   â”‚   â””â”€â”€ react_agent.py          # ReAct agent
â”‚   â”‚
â”‚   â”œâ”€â”€ rag_enhanced/                # Enhanced RAG
â”‚   â”‚   â””â”€â”€ family_aware_retriever.py  # Family search
â”‚   â”‚
â”‚   â”œâ”€â”€ document_families/           # Document families
â”‚   â”‚   â””â”€â”€ family_manager.py       # Family management
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/                       # Agent tools
â”‚   â”‚   â””â”€â”€ lazy_search_tools.py    # Search tools
â”‚   â”‚
â”‚   â””â”€â”€ chunking/                    # Text chunking
â”‚       â””â”€â”€ adaptive_chunker.py     # Smart chunking
â”‚
â”œâ”€â”€ frontend/                         # React Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx                  # Main app
â”‚   â”‚   â”œâ”€â”€ pages/                   # Pages
â”‚   â”‚   â”‚   â”œâ”€â”€ admin/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ EnhancedWebScrapingPage.jsx
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ DocumentManagementPage.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatPage.jsx
â”‚   â”‚   â”‚   â””â”€â”€ auth/
â”‚   â”‚   â”‚       â””â”€â”€ LoginPage.jsx
â”‚   â”‚   â”œâ”€â”€ components/              # Components
â”‚   â”‚   â”‚   â””â”€â”€ layout/
â”‚   â”‚   â”‚       â””â”€â”€ Sidebar.jsx
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â””â”€â”€ api.js               # API client
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ .env                              # Environment variables
â”œâ”€â”€ requirements.txt                  # Python dependencies
â””â”€â”€ README.md                         # Documentation
```

---

## Detailed Workflows

### Workflow 1: Complete Web Scraping Process

**Step-by-Step**:

1. **User Initiates Scraping** (Frontend)

   - File: `frontend/src/pages/admin/EnhancedWebScrapingPage.jsx`
   - User clicks "Scrape" button for a source
   - Sends POST request to `/api/enhanced-web-scraping/scrape-enhanced`

2. **API Receives Request** (Backend)

   - File: `backend/routers/enhanced_web_scraping_router.py`
   - Validates user is admin
   - Creates job ID for tracking
   - Calls `enhanced_scrape_source()`

3. **Scraping Orchestration** (Agent)

   - File: `Agent/web_scraping/enhanced_processor.py`
   - Function: `enhanced_scrape_source()`
   - Gets source details from database
   - Selects appropriate site scraper

4. **Site-Specific Scraping**

   - File: `Agent/web_scraping/site_scrapers/moe_scraper.py` (or ugc/aicte)
   - Function: `scrape_documents()`
   - Visits website
   - Finds document links using CSS selectors
   - Handles pagination (sliding window)

5. **Document Download**

   - File: `Agent/web_scraping/enhanced_processor.py`
   - Function: `_download_and_process_document()`
   - Downloads PDF/DOC file
   - Saves to temp directory

6. **Deduplication Check**

   - File: `Agent/web_scraping/document_identity_manager.py`
   - Function: `is_duplicate()`
   - Checks 3 levels:
     - URL match
     - Content hash (SHA256)
     - Normalized URL
   - If duplicate: Skip or update

7. **Upload to S3**

   - File: `backend/utils/supabase_storage.py`
   - Function: `upload_to_supabase()`
   - Uploads file to Supabase bucket
   - Returns public URL

8. **Text Extraction**

   - File: `backend/utils/text_extractor.py`
   - Function: `extract_text()`
   - Extracts text from PDF/DOC
   - Falls back to OCR if needed

9. **Metadata Extraction** (AI)

   - File: `Agent/metadata/extractor.py`
   - Function: `extract_metadata()`
   - Uses Google Gemini (gemma-3-12b)
   - Extracts: title, department, type, date, summary, keywords
   - Validates quality

10. **Family Detection**

    - File: `Agent/document_families/family_manager.py`
    - Function: `find_or_create_family()`
    - Checks if document belongs to existing family
    - Creates new family or adds to existing
    - Updates version numbers

11. **Save to Database**

    - File: `Agent/web_scraping/enhanced_processor.py`
    - Creates Document record
    - Creates DocumentMetadata record
    - Links to DocumentFamily
    - Sets approval status

12. **Return Results**

    - File: `backend/routers/enhanced_web_scraping_router.py`
    - Returns stats:
      - documents_new
      - documents_updated
      - documents_duplicate
      - families_created
      - families_updated

13. **Frontend Updates**
    - File: `frontend/src/pages/admin/EnhancedWebScrapingPage.jsx`
    - Displays success toast with stats
    - Refreshes document list
    - Updates counters

**Files Touched** (in order):

```
1. EnhancedWebScrapingPage.jsx (UI)
2. enhanced_web_scraping_router.py (API)
3. enhanced_processor.py (Orchestrator)
4. moe_scraper.py (Site scraper)
5. document_identity_manager.py (Dedup)
6. supabase_storage.py (S3 upload)
7. text_extractor.py (Text extraction)
8. extractor.py (Metadata AI)
9. family_manager.py (Families)
10. database.py (Save)
11. enhanced_web_scraping_router.py (Response)
12. EnhancedWebScrapingPage.jsx (Display)
```

---

### Workflow 2: AI Chat with Documents (RAG)

**Step-by-Step**:

1. **User Asks Question** (Frontend)

   - File: `frontend/src/pages/ChatPage.jsx`
   - User types: "What is the Indo-Norwegian program?"
   - Sends POST to `/api/chat/query`

2. **Chat API Receives Request** (Backend)

   - File: `backend/routers/chat_router.py`
   - Validates user authentication
   - Gets user role and institution
   - Calls RAG agent

3. **RAG Agent Initialization** (Agent)

   - File: `Agent/rag_agent/react_agent.py`
   - Function: `PolicyRAGAgent.__init__()`
   - Initializes LLM (OpenRouter - Llama 3.3 70B)
   - Sets up search tools

4. **Agent Processes Query**

   - File: `Agent/rag_agent/react_agent.py`
   - Function: `query()`
   - Analyzes question
   - Decides which tool to use

5. **Search Tool Invoked**

   - File: `Agent/tools/lazy_search_tools.py`
   - Function: `search_documents_lazy()`
   - Applies role-based filters

6. **Check Embedding Status**

   - File: `Agent/tools/lazy_search_tools.py`
   - Queries database for unembed documents
   - Filters by user access permissions

7. **Metadata-Based Ranking** (If unembed docs exist)

   - File: `Agent/tools/lazy_search_tools.py`
   - Uses BM25 to rank documents by metadata
   - Ranks based on: title, keywords, summary
   - Selects top 3 most relevant

8. **Lazy Embedding** (On-demand)

   - File: `Agent/lazy_rag/lazy_embedder.py`
   - Function: `embed_document()`
   - Chunks text (adaptive chunking)
   - Generates embeddings (BGE-M3)
   - Stores in pgvector

9. **Vector Search**

   - File: `Agent/vector_store/pgvector_store.py`
   - Function: `search()`
   - Generates query embedding
   - Searches pgvector (cosine similarity)
   - Applies role-based filters
   - Returns top matches

10. **Hybrid Search** (Optional)

    - File: `Agent/retrieval/hybrid_retriever.py`
    - Combines vector search (70%) + BM25 (30%)
    - Merges and ranks results

11. **Reranking** (Optional)

    - File: `Agent/metadata/reranker.py`
    - Function: `rerank()`
    - Uses LLM to rerank by relevance
    - Returns top 5 results

12. **Answer Generation**

    - File: `Agent/rag_agent/react_agent.py`
    - LLM receives: question + relevant chunks
    - Generates answer with citations
    - Formats response

13. **Return to User**

    - File: `backend/routers/chat_router.py`
    - Returns formatted answer
    - Includes source citations
    - Document IDs and approval status

14. **Frontend Displays Answer**
    - File: `frontend/src/pages/ChatPage.jsx`
    - Shows answer with citations
    - Links to source documents
    - Displays approval badges

**Files Touched** (in order):

```
1. ChatPage.jsx (UI)
2. chat_router.py (API)
3. react_agent.py (RAG agent)
4. lazy_search_tools.py (Search)
5. lazy_embedder.py (Embedding)
6. adaptive_chunker.py (Chunking)
7. bge_embedder.py (Embeddings)
8. pgvector_store.py (Vector search)
9. reranker.py (Reranking)
10. react_agent.py (Answer generation)
11. chat_router.py (Response)
12. ChatPage.jsx (Display)
```

---

### Workflow 3: Document Upload & Processing

**Step-by-Step**:

1. **User Uploads File** (Frontend)

   - File: `frontend/src/pages/admin/DocumentManagementPage.jsx`
   - User selects PDF file
   - Fills metadata form
   - Clicks "Upload"

2. **API Receives File** (Backend)

   - File: `backend/routers/document_router.py`
   - Function: `upload_document()`
   - Validates file type and size
   - Checks user permissions

3. **Save Temporary File**

   - File: `backend/routers/document_router.py`
   - Saves to temp directory
   - Generates unique filename

4. **Text Extraction**

   - File: `backend/utils/text_extractor.py`
   - Function: `extract_text()`
   - For PDF: Uses PyPDF2
   - If low quality: Falls back to Tesseract OCR
   - For DOCX: Uses python-docx
   - Returns extracted text

5. **Upload to S3**

   - File: `backend/utils/supabase_storage.py`
   - Function: `upload_to_supabase()`
   - Uploads to Supabase bucket
   - Returns public URL

6. **Metadata Extraction** (AI)

   - File: `Agent/metadata/extractor.py`
   - Function: `extract_metadata()`
   - Uses Gemini (gemma-3-12b)
   - Extracts structured metadata
   - Validates quality

7. **Create Database Records**

   - File: `backend/routers/document_router.py`
   - Creates Document record:
     - filename, file_type, s3_url
     - extracted_text, uploader_id
     - approval_status, visibility_level
   - Creates DocumentMetadata record:
     - title, department, document_type
     - date_published, keywords, summary

8. **Family Assignment** (Optional)

   - File: `Agent/document_families/family_manager.py`
   - Checks for existing family
   - Assigns to family or creates new

9. **Return Success**

   - File: `backend/routers/document_router.py`
   - Returns document ID
   - Returns metadata

10. **Frontend Updates**
    - File: `frontend/src/pages/admin/DocumentManagementPage.jsx`
    - Shows success message
    - Refreshes document list
    - Clears form

**Files Touched** (in order):

```
1. DocumentManagementPage.jsx (UI)
2. document_router.py (API)
3. text_extractor.py (Text extraction)
4. supabase_storage.py (S3 upload)
5. extractor.py (Metadata AI)
6. family_manager.py (Families)
7. database.py (Save)
8. document_router.py (Response)
9. DocumentManagementPage.jsx (Display)
```

---

## Key Technologies Explained

### 1. pgvector (Vector Database)

**What**: PostgreSQL extension for vector similarity search
**Why**: Fast semantic search without separate vector DB
**How**: Stores 1024-dim embeddings, uses cosine similarity

### 2. BGE-M3 (Embeddings)

**What**: Multilingual embedding model
**Why**: Supports 100+ languages including Hindi
**How**: Converts text to 1024-dim vectors

### 3. Lazy RAG

**What**: Embed documents on-demand
**Why**: Saves compute, only embed what's needed
**How**: Rank by metadata, embed top matches

### 4. Document Families

**What**: Group related documents
**Why**: Track versions, avoid duplicates
**How**: Similarity matching + version tracking

### 5. Hybrid Search

**What**: Vector + keyword search
**Why**: Better accuracy than either alone
**How**: 70% vector + 30% BM25

### 6. Role-Based Access Control (RBAC)

**What**: Different permissions per role
**Why**: Security and privacy
**How**: Filter queries by role and institution

---

## Configuration Files

### `.env` - Environment Variables

```env
# Database
DATABASE_HOSTNAME=aws-1-ap-south-1.pooler.supabase.com
DATABASE_PORT=5432
DATABASE_NAME=postgres
DATABASE_USERNAME=postgres.ppqdbqzlfxddfroxlycx
DATABASE_PASSWORD=suyashgandu

# AI Models
GOOGLE_API_KEY=AIzaSyDkCCqQdgGtrd2t1yGjCJ4zv4QmNNjn93w
OPENROUTER_API_KEY=sk-or-v1-288a791142fc9234...
OPENROUTER_MODEL=meta-llama/llama-3.3-70b-instruct:free

# LLM Providers
METADATA_LLM_PROVIDER=gemini          # gemma-3-12b (14,400/day)
RAG_LLM_PROVIDER=openrouter           # Llama 3.3 (200/day)
RERANKER_PROVIDER=openrouter          # Llama 3.3 (200/day)

# Storage
SUPABASE_URL=https://ppqdbqzlfxddfroxlycx.supabase.co
SUPABASE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
SUPABASE_BUCKET_NAME=Docs

# Quality Control
DELETE_DOCS_WITHOUT_METADATA=false
REQUIRE_TITLE=false
REQUIRE_SUMMARY=false
```

### `requirements.txt` - Python Dependencies

```txt
fastapi==0.104.1
uvicorn==0.24.0
sqlalchemy==2.0.23
psycopg2-binary==2.9.9
pgvector==0.2.3
langchain==0.1.0
langchain-google-genai==0.0.6
langchain-openai==0.0.2
sentence-transformers==2.2.2
rank-bm25==0.2.2
python-multipart==0.0.6
python-jose==3.3.0
passlib==1.7.4
bcrypt==4.1.1
httpx==0.25.2
beautifulsoup4==4.12.2
PyPDF2==3.0.1
python-docx==1.1.0
pytesseract==0.3.10
Pillow==10.1.0
```

---

## Performance Metrics

### Scraping Performance:

- **Speed**: ~10 documents/minute
- **Quota**: 14,400 documents/day
- **Deduplication**: 33.5% duplicates found
- **Success Rate**: 95%+

### Search Performance:

- **First search**: 2-5 seconds (with lazy embedding)
- **Subsequent searches**: <1 second
- **Accuracy**: 85% (family-aware retrieval)
- **Quota**: 200 queries/day

### Storage:

- **Database**: PostgreSQL (Supabase)
- **Files**: Supabase S3 bucket
- **Embeddings**: pgvector (in PostgreSQL)
- **Total docs**: 1779+ documents

---

## Troubleshooting Guide

### Issue 1: Scraping Fails

**Check**:

1. `Agent/web_scraping/enhanced_processor.py` - Logs
2. Site scraper (moe/ugc/aicte) - CSS selectors
3. Metadata extraction - API quota

### Issue 2: Chat Not Working

**Check**:

1. `Agent/rag_agent/react_agent.py` - LLM initialization
2. `.env` - RAG_LLM_PROVIDER setting
3. OpenRouter API key validity

### Issue 3: No Search Results

**Check**:

1. `Agent/vector_store/pgvector_store.py` - Embeddings exist
2. Role-based filters - User permissions
3. Document approval status

### Issue 4: Metadata Extraction Fails

**Check**:

1. `Agent/metadata/extractor.py` - API errors
2. Google API quota (14,400/day)
3. Fallback to OpenRouter

---

## Summary

**BEACON** is a comprehensive document management and RAG system with:

âœ… **Automated web scraping** from government sites
âœ… **AI-powered metadata extraction** (14,400/day quota)
âœ… **Intelligent chat** with document citations
âœ… **Role-based access control** for security
âœ… **Document families** for version tracking
âœ… **Lazy RAG** for efficient embedding
âœ… **Hybrid search** for better accuracy
âœ… **Multilingual support** (100+ languages)

**Total Capacity**: 14,600 operations/day
**Tech Stack**: FastAPI + React + PostgreSQL + AI
**Status**: Production-ready âœ…

---

_For questions or support, refer to individual file documentation or contact the development team._
