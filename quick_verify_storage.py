"""
Quick verification - checks the scraping code directly without loading heavy models
"""
import os
from dotenv import load_dotenv

load_dotenv()

print("=" * 80)
print("QUICK STORAGE VERIFICATION")
print("=" * 80)

# Check 1: Environment
print("\n‚úÖ Supabase Configuration:")
print(f"   URL: {os.getenv('SUPABASE_URL')}")
print(f"   Bucket: {os.getenv('SUPABASE_BUCKET_NAME', 'Docs')}")

# Check 2: Read the scraping code directly
print("\n‚úÖ Checking Scraping Code...")
with open('Agent/web_scraping/enhanced_processor.py', 'r', encoding='utf-8') as f:
    code = f.read()
    
    checks = {
        "Imports upload_to_supabase": "from backend.utils.supabase_storage import upload_to_supabase" in code,
        "Calls upload_to_supabase": "s3_url = upload_to_supabase(tmp_path, unique_filename)" in code,
        "Creates Document with s3_url": "s3_url=s3_url" in code,
        "Saves source_url": "source_url=doc_info['url']" in code,
        "Commits to database": "db.commit()" in code,
    }
    
    all_passed = True
    for check, passed in checks.items():
        status = "‚úÖ" if passed else "‚ùå"
        print(f"   {status} {check}")
        if not passed:
            all_passed = False

# Check 3: Verify storage utility
print("\n‚úÖ Checking Storage Utility...")
with open('backend/utils/supabase_storage.py', 'r', encoding='utf-8') as f:
    code = f.read()
    
    checks = {
        "Uploads to bucket": "supabase.storage.from_(BUCKET_NAME).upload" in code,
        "Returns public URL": "get_public_url(filename)" in code,
        "Uses correct bucket": "BUCKET_NAME = os.getenv" in code,
    }
    
    for check, passed in checks.items():
        status = "‚úÖ" if passed else "‚ùå"
        print(f"   {status} {check}")
        if not passed:
            all_passed = False

# Check 4: Verify database schema
print("\n‚úÖ Checking Database Schema...")
with open('backend/database.py', 'r', encoding='utf-8') as f:
    code = f.read()
    
    # Find Document class
    if "class Document(Base):" in code:
        print("   ‚úÖ Document model found")
        
        # Check for required fields
        fields = {
            "s3_url": "s3_url = Column(String)" in code,
            "source_url": "source_url = Column(String" in code,
            "extracted_text": "extracted_text = Column(Text)" in code,
            "filename": "filename = Column(String" in code,
        }
        
        for field, exists in fields.items():
            status = "‚úÖ" if exists else "‚ùå"
            print(f"   {status} {field} field exists")
            if not exists:
                all_passed = False
    else:
        print("   ‚ùå Document model NOT FOUND!")
        all_passed = False

# Final verdict
print("\n" + "=" * 80)
if all_passed:
    print("‚úÖ ALL CHECKS PASSED!")
    print("\nYour scraping system WILL:")
    print("  1. ‚úÖ Download documents from URLs")
    print("  2. ‚úÖ Upload to Supabase storage bucket 'Docs'")
    print("  3. ‚úÖ Save s3_url to database")
    print("  4. ‚úÖ Save source_url to database")
    print("  5. ‚úÖ Extract and store metadata")
    print("\nüöÄ READY TO SCRAPE!")
    print("\nExpected storage location:")
    print(f"  {os.getenv('SUPABASE_URL')}/storage/v1/object/public/Docs/scraped_TIMESTAMP_FILENAME.pdf")
else:
    print("‚ùå SOME CHECKS FAILED!")
    print("\n‚ö†Ô∏è  Please review the failed checks above")

print("=" * 80)
